{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from dimensional_structure.EFA_plots import get_communality, get_adjusted_communality\n",
    "from selfregulation.utils.plot_utils import format_num\n",
    "from selfregulation.utils.r_to_py_utils import get_attr\n",
    "from selfregulation.utils.result_utils import load_results\n",
    "from selfregulation.utils.utils import get_recent_dataset, get_retest_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "results = load_results(get_recent_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(s, survey_val, task_val):\n",
    "    num1 = format_num(survey_val, 3)\n",
    "    num2 = format_num(task_val, 3)\n",
    "    print('%s:\\nsurvey: %s\\ntask: %s' % (s, num1, num2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Paper Numeric Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFA bootstrapped factor reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_boot_sds = results['survey'].EFA.get_boot_stats()['sds'].mean().mean()\n",
    "task_boot_sds = results['task'].EFA.get_boot_stats()['sds'].mean().mean()\n",
    "\n",
    "print_result('Loading Average SDs across bootstrap samples', survey_boot_sds, task_boot_sds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Expalined by EFA models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the variance explained for each factor analytic model. Extracted from the fa function from R's Psych package. Equivalent to taking the mean of individual variable communalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variance explained by survey and task EFA\n",
    "survey_EFA_rout = results['survey'].EFA.results['factor_tree_Rout_oblimin'][12]\n",
    "survey_EFA_cummvar = get_attr(survey_EFA_rout, 'Vaccounted')[2,-1]\n",
    "\n",
    "task_EFA_rout = results['task'].EFA.results['factor_tree_Rout_oblimin'][5]\n",
    "task_EFA_cummvar = get_attr(task_EFA_rout, 'Vaccounted')[2,-1]\n",
    "\n",
    "print_result('Variance Expalined', survey_EFA_cummvar, task_EFA_cummvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Adjusted\" variable explained taking into account the reliability of each measure. We first drop out variables with very low reliability (<.2) before conducting this analysis as they can have extreme effects on the communality of a variable. \n",
    "\n",
    "To ensure that the effect of adjustment isn't explained by dropping out these variables we show the variance explained for the reliable measure subsets. It is clear that subsetting is now meaningfully changing the variance explained values for surveys or tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_adjusted(results):\n",
    "    communality = get_communality(results.EFA)\n",
    "    retest_data = get_retest_data(dataset=results.dataset.replace('Complete','Retest'))\n",
    "    # reorder data in line with communality\n",
    "    retest_data = retest_data.loc[communality.index]\n",
    "    adjusted, *_ = get_adjusted_communality(communality, retest_data)\n",
    "    return adjusted\n",
    "\n",
    "survey_adjusted = return_adjusted(results['survey'])\n",
    "task_adjusted = return_adjusted(results['task'])\n",
    "print_result('Adjusted Variance Expalined', survey_adjusted.mean(), task_adjusted.mean())\n",
    "\n",
    "# also calculate the unadjusted communality for the remaining variables\n",
    "unadjusted_survey = get_communality(results['survey'].EFA)[survey_adjusted.index].mean()\n",
    "unadjusted_task = get_communality(results['task'].EFA)[task_adjusted.index].mean()\n",
    "print_result('Unadjusted Variance Expalined for reliable subset', \n",
    "             unadjusted_survey, unadjusted_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['survey'].EFA.get_scores().corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['task'].EFA.get_scores().corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_summary(predictions, scores='scores_cv'):\n",
    "    R2s = []\n",
    "    for k,v in predictions.items():\n",
    "        R2s.append(v[scores][0]['R2'])\n",
    "    return np.mean(R2s), np.min(R2s), np.max(R2s), np.array(R2s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in ['ridge', 'svm']:\n",
    "    for EFA in [True, False]:\n",
    "        EFA_tag = 'Ontology' if EFA else 'Raw'\n",
    "        print('*'*30, EFA_tag, classifier, '*'*30)\n",
    "        # Load the prediction resultings using ridge regression with EFA factors as predictors\n",
    "        survey_prediction_results = results['survey'].load_prediction_object(classifier=classifier, EFA=EFA)['data']\n",
    "        task_prediction_results = results['task'].load_prediction_object(classifier=classifier, EFA=EFA)['data']\n",
    "\n",
    "        print ('Mean, Min, Max Performance')\n",
    "        survey_out = get_pred_summary(survey_prediction_results)\n",
    "        task_out = get_pred_summary(task_prediction_results)\n",
    "        print_result('Prediction Performance', survey_out[:-1], task_out[:-1])\n",
    "\n",
    "        # evaluate degree of overestimation when performing insample prediction\n",
    "        survey_insample = get_pred_summary(survey_prediction_results, scores='scores_insample')[-1]\n",
    "        task_insample = get_pred_summary(task_prediction_results, scores='scores_insample')[-1]\n",
    "\n",
    "        survey_exaggeration = np.mean(survey_insample-survey_out[-1])\n",
    "        task_exaggeration = np.mean(task_insample-task_out[-1])\n",
    "        print_result('\\nInsample exaggeration Absolute', survey_exaggeration, task_exaggeration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
