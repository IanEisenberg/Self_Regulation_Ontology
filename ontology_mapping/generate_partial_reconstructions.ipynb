{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from reconstruction_utils import run_kNeighbors, reorder_FA\n",
    "from selfregulation.utils.utils import get_recent_dataset\n",
    "from selfregulation.utils.result_utils import load_results, get_info\n",
    "from selfregulation.utils.r_to_py_utils import psychFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_close_reconstruction(results, drop_regex, num_available_measures=None,\n",
    "                             pseudo_pop_size=60, n_reps=100, \n",
    "                             k_list=None, EFA_rotation='oblimin', \n",
    "                             metric='correlation',\n",
    "                             verbose=True,\n",
    "                             weightings = ['distance']):\n",
    "    \"\"\"\n",
    "    Extension of k_nearest_reconstruction that uses a omnipotent \"closeness\" algorithm \n",
    "    to select a subset of variables\n",
    "    \"\"\"\n",
    "    def run_EFA(data, c, rotation, orig_loading):\n",
    "        fa, out = psychFA(data, c, rotate=EFA_rotation)\n",
    "        loadings = pd.DataFrame(out['loadings'], index=data.columns)\n",
    "        loadings = reorder_FA(orig_loadings, loadings)\n",
    "        return loadings\n",
    "    \n",
    "    def get_closest(data, target, n_tasks=5, metric='correlation'):\n",
    "        index = data.columns.get_loc(target)\n",
    "        distances = squareform(pdist(data.T, metric=metric))\n",
    "        sort_vars = data.columns[np.argsort(distances[index])]\n",
    "        # get closest tasks until tasks are filled up\n",
    "        tasks = set()\n",
    "        for var in sort_vars:\n",
    "            task, *_ = var.split('.')\n",
    "            tasks.add(task)\n",
    "            if len(tasks) == n_tasks:\n",
    "                break\n",
    "        # get variables from tasks\n",
    "        neighbors = data.filter(regex='|'.join(tasks)).columns\n",
    "        return neighbors\n",
    "    c = results.EFA.results['num_factors']\n",
    "    orig_loadings = results.EFA.get_loading(c, rotate=EFA_rotation)\n",
    "    full_data = results.data\n",
    "    drop_vars = list(full_data.filter(regex=drop_regex).columns)\n",
    "    subset = full_data.drop(drop_vars, axis=1)\n",
    "    full_loadings = run_EFA(subset, c, EFA_rotation, orig_loadings)\n",
    "    if full_loadings is None:\n",
    "        return\n",
    "    var_reconstructions = pd.DataFrame()\n",
    "    for var in drop_vars:\n",
    "        # imagine we have a good estimate of one measure tomap is related to\n",
    "        target = full_data.corr()[var].drop(drop_vars).idxmax()\n",
    "        # get a neighborhood around that target\n",
    "        available_vars = get_closest(full_loadings.T, target, n_tasks=num_available_measures,\n",
    "                                    metric=metric)\n",
    "\n",
    "        # get dataset and loadings\n",
    "        data = full_data.loc[:, set(available_vars) | set(drop_vars)]\n",
    "        loadings = full_loadings.loc[available_vars,:]\n",
    "        for rep in range(n_reps):\n",
    "            random_subset = data.sample(pseudo_pop_size)\n",
    "            distances = pd.DataFrame(squareform(pdist(random_subset.T, metric=metric)), \n",
    "                                     index=data.columns, \n",
    "                                     columns=data.columns).drop(drop_vars, axis=1)\n",
    "            tmp_reconstruction = run_kNeighbors(distances, loadings, [var], weightings, \n",
    "                                                [min(loadings.shape[0], 13)])\n",
    "            tmp_reconstruction['label'] = \"closest_reconstruction\"\n",
    "            tmp_reconstruction['num_available_measures'] = num_available_measures\n",
    "            tmp_reconstruction['rep'] = rep\n",
    "            var_reconstructions = pd.concat([var_reconstructions, tmp_reconstruction])\n",
    "    return var_reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EFA_rotation='oblimin'\n",
    "dataset = get_recent_dataset()\n",
    "\n",
    "results = load_results(dataset)['task']\n",
    "output_dir = path.join(get_info('results_directory'),\n",
    "                       'ontology_reconstruction', dataset, results.ID, EFA_rotation)\n",
    "measure_list = np.unique([i.split('.')[0] for i in results.data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reconstruction = pd.DataFrame()\n",
    "regex_list = ['^'+m for m in measure_list]\n",
    "for num_available_measures in range(1,11):\n",
    "    for pop_size in [30, 50, 100, 400]:\n",
    "        for drop_regex in regex_list:\n",
    "            var_reconstruction = k_nearest_close_reconstruction(results, drop_regex, num_available_measures, \n",
    "                                          pseudo_pop_size=pop_size, n_reps=50)\n",
    "            if var_reconstruction is None:\n",
    "                continue\n",
    "            var_reconstruction['pop_size'] = pop_size\n",
    "            full_reconstruction = pd.concat([full_reconstruction, var_reconstruction])\n",
    "full_reconstruction = full_reconstruction.sort_values(by='var')\n",
    "full_reconstruction.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of components not specified, using BIC determined #\n"
     ]
    }
   ],
   "source": [
    "# get reconstruction scores\n",
    "loadings = results.EFA.get_loading()\n",
    "loadings\n",
    "scores = []\n",
    "for i, row in full_reconstruction.iterrows():\n",
    "    var = row['var']\n",
    "    onto_embedding = loadings.loc[var]\n",
    "    estimated_embedding = row[onto_embedding.index]\n",
    "    score = np.corrcoef(list(estimated_embedding), \n",
    "                          list(onto_embedding))[0,1]\n",
    "    scores.append(score)\n",
    "full_reconstruction.loc[:, 'corr_score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = full_reconstruction.groupby(['var','pop_size','num_available_measures']).corr_score.agg([np.mean, np.std]).reset_index()\n",
    "summary.to_pickle(path.join(output_dir, 'KNNRclosest_correlation_summary.pkl' ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
