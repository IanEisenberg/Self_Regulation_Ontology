{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Reconstructing out-of-sample DVs\n",
    "\n",
    "Given a quantitative ontology, or psychological space, that DVs can be projected into, how can we determine the embedding of new variables?\n",
    "\n",
    "Currently, our embedding is determined by factor analysis. Thus ontological embedding are only known for the DVs entered into the original model. How could we extend this?\n",
    "\n",
    "One possibility is measuring new variables in the same population that completed our original battery. After doing this we could either (1) run the model anew, or (2) use linear regression to map the already discovered factors onto the new variables. The former is better, but results in small changes to the actual factors with each new variable. The latter method ensures that our factors stay the same. Neither is scalable, however, as we do not, in general, have access to a constant population that can be remeasured whenever new measures come into the picture.\n",
    "\n",
    "Another possibility that works with new populations requires that the new population completes the entire battery used to estimate the original factors, in addition to whatever new variables are of interest. Doing so allows the calculation of factor scores for this new population based on the original model, which can then be mapped to the new measures of interest. This allows researchers to capitalize on the original model (presumably fit on more subjects than the new study), while expanding the ontology. Problems exist here, however.\n",
    "- The most obvious problem is that you have to measure the new sample on the entire battery used to fit the original EFA model. Given that this takes many hours (the exact number depending on whether tasks, surveys or both are used), this is exceedingly impractical. In our case we did have our new fMRI sample take the entire battery (or at least a subset of participants), so this problem isn't as relevant\n",
    "- Still problems remain. If N is small, the estimates of the ontological embeddings for new DVs are likely unstable.\n",
    "\n",
    "This latter problem necessitates some quantitative exploration. This notebook simulates the issue by:\n",
    "1. Removing a DV from the original ontology dataset\n",
    "2. Performing EFA on this subset\n",
    "3. Using linear regression to map these EFA factors to the left out variable\n",
    "(3) is performed on smaller population sizes to reflect the reality of most studies (including ours) and is repeated to get a sense of the mapping's variability\n",
    "\n",
    "This simulates the ideal case of mapping a new variable by measuring the entire ontological battery. We also use K-nearest-neighbor regression to map new variables into the space with fewer variables. Doing this proceeds as follows:\n",
    "1. Remove a DV from the original ontology dataset\n",
    "2. Perform EFA\n",
    "3. Create a distance matrix between each DV and every other DV\n",
    "\n",
    "\n",
    "### Small issues not currently addressed\n",
    "\n",
    "- The EFA model is fit on the entire population. An even more stringent simulation would subset the subjects used in the \"new study\" and fit the EFA model on a completely independent group. I tried this once - the factor scores hardly differed. In addition, I want the EFA model to be as well-powered as possible, as that will be the reality for this method moving forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from os import makedirs, path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "from sklearn.preprocessing import normalize\n",
    "from ontology_mapping.reconstruction_utils import (load_files,\n",
    "                                                   combine_files,\n",
    "                                                   update_files,\n",
    "                                                   normalize_reconstruction,\n",
    "                                                   get_reconstruction_results, \n",
    "                                                   linear_reconstruction,\n",
    "                                                   k_nearest_reconstruction,\n",
    "                                                   CV_predict,\n",
    "                                                   summarize_k,\n",
    "                                                   run_reconstruction)\n",
    "from selfregulation.utils.result_utils import load_results\n",
    "from selfregulation.utils.utils import get_recent_dataset, get_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.metrics.classification.UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argparse\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-pop_sizes', nargs='+', default=[30, 50, 100, 400], type=int)\n",
    "    parser.add_argument('-n_reps', default=250, type=int)\n",
    "    parser.add_argument('-n_measures', default=None, type=int)\n",
    "    parser.add_argument('-result_subset', default='task')\n",
    "    parser.add_argument('-rerun', action='store_true')\n",
    "    parser.add_argument('-no_append', action='store_true')\n",
    "    parser.add_argument('-EFA_rotation', default='oblimin')\n",
    "    parser.add_argument('-knn_metric', default='correlation')\n",
    "    parser.add_argument('-verbose', action='store_true')\n",
    "    parser.add_argument('-no_save', action='store_true')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    pop_sizes = args.pop_sizes\n",
    "    n_reps = args.n_reps\n",
    "    n_measures = args.n_measures\n",
    "    result_subset = args.result_subset\n",
    "    rerun = args.rerun\n",
    "    append = not args.no_append\n",
    "    knn_metric = args.knn_metric\n",
    "    EFA_rotation = args.EFA_rotation\n",
    "    verbose = args.verbose\n",
    "    dataset = get_recent_dataset()\n",
    "    save = not args.no_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "np.random.seed(12412)\n",
    "results = load_results(dataset)[result_subset]\n",
    "c = results.EFA.get_c()\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {'Ridge': Ridge(fit_intercept=False),\n",
    "               'LR': LinearRegression(fit_intercept=False)}\n",
    "# get output dir to store results\n",
    "output_dir = path.join(get_info('results_directory'),\n",
    "                       'ontology_reconstruction', dataset, results.ID, EFA_rotation)\n",
    "makedirs(output_dir, exist_ok=True)\n",
    "# get plot dir to store plots\n",
    "plot_dir = path.join(output_dir, 'Plots')\n",
    "makedirs(plot_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random subset of variables to perform the calculation on if n_vars is set\n",
    "measures = np.unique([i.split('.')[0] for i in results.data.columns])\n",
    "if n_measures is not None:\n",
    "    measure_list = np.random.choice(measures, n_measures, replace=False)\n",
    "else:\n",
    "    measure_list = measures\n",
    "# get all variables from selected tasks\n",
    "var_list = results.data.filter(regex='|'.join(measure_list)).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run simulation for every variable at different population sizes. \n",
    "\n",
    "That is, do the following:\n",
    "\n",
    "1. take a variable (say stroop incongruent-congruent RT), remove it from the data matrix\n",
    "2. Run EFA on the data matrix composes of the 522 (subject) x N-1 (variable) data matrix\n",
    "3. Calculate factor scores for all 522 subjects\n",
    "4. Select a subset of \"pop_size\" to do an \"ontological mapping\". That is, pretend that these subjects did the whole battery (missing the one variable) *and then* completed one more task. The idea is we want to do a mapping from those subject's factor scores to the new variable\n",
    "   1. We can do a linear mapping (regression) from the ontological scores to the output variable\n",
    "   2. We can do a k-nearest neighbor interpolation, where we say the unknown ontological factor is a blend of the \"nearest\" variables in the dataset\n",
    "5. Repeat (4) a number of times to get a sense for the accuracy and variability of that mapping\n",
    "6. Compare the estimated ontological scores for the held out var (stroop incongruent-congruent) to the original \"correct\" ontological mapping (that would have been obtained if the variable was included in the original data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbor Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "verbose=True\n",
    "for name, independent_flag in [('KNNR', False), ('KNNRind', True)]:\n",
    "    k_list = list(range(1,20))\n",
    "    basename = path.join(output_dir, '%s_%s-*' % (name, knn_metric))\n",
    "    files = glob(basename)\n",
    "    if name == \"KNNRind\":\n",
    "        pops_to_use = [i for i in pop_sizes if i < 100]\n",
    "    else:\n",
    "        pops_to_use = pop_sizes\n",
    "    updated, k_reconstructions = run_reconstruction(results, \n",
    "                                                   measure_list, \n",
    "                                                   pops_to_use, \n",
    "                                                   n_reps, \n",
    "                                                   k_nearest_reconstruction,\n",
    "                                                   previous_files=files, \n",
    "                                                   append=append, \n",
    "                                                   verbose=verbose, \n",
    "                                                   k_list=k_list, \n",
    "                                                   metric=knn_metric,\n",
    "                                                   independent_EFA=independent_flag,\n",
    "                                                   EFA_rotation=EFA_rotation)\n",
    "    for measure in updated:\n",
    "        df = k_reconstructions[measure]\n",
    "        if save:\n",
    "            df.to_pickle(basename[:-1]+'%s.pkl' % measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "clfs = {'Linear': LinearRegression(fit_intercept=False),\n",
    "       'RidgeCV': RidgeCV(fit_intercept=False, cv=10)}\n",
    "linear_reconstructions = {}\n",
    "for clf_name, clf in clfs.items():\n",
    "    \n",
    "    basename = path.join(output_dir, 'linear-%s_reconstruct*' % clf_name)\n",
    "    files = glob(basename)\n",
    "    updated, reconstruction = run_reconstruction(results, \n",
    "                                                   measure_list, \n",
    "                                                   pop_sizes, \n",
    "                                                   n_reps, \n",
    "                                                   linear_reconstruction,\n",
    "                                                   previous_files=files, \n",
    "                                                   append=append, \n",
    "                                                   verbose=verbose, \n",
    "                                                   clf=clf,\n",
    "                                                   EFA_rotation=EFA_rotation)\n",
    "    for measure in updated:\n",
    "        df = reconstruction[measure]\n",
    "        if save:\n",
    "            df.to_pickle(basename[:-1]+'-%s.pkl' % measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbor Partial Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_available_measures in range(5,len(measures),5):\n",
    "    # repeat with different samples of measures\n",
    "    for sample_repeats in range(50):\n",
    "        if verbose and sample_repeats%5==0:\n",
    "            print('SAMPLE %s FOR %s MEASURES' % (sample_repeats, num_available_measures))\n",
    "        basename = path.join(output_dir, '%s_%s-*' % ('KNNRpartial', knn_metric))\n",
    "        files = glob(basename)\n",
    "        k_list=[13]\n",
    "        updated, k_reconstructions = run_reconstruction(results, \n",
    "                                                       measure_list, \n",
    "                                                       pop_sizes, \n",
    "                                                       n_reps=10, \n",
    "                                                       recon_fun=k_nearest_reconstruction,\n",
    "                                                       previous_files=files, \n",
    "                                                       append=append, \n",
    "                                                       verbose=False, \n",
    "                                                       k_list=k_list, \n",
    "                                                       metric=knn_metric,\n",
    "                                                       independent_EFA=False,\n",
    "                                                       EFA_rotation=EFA_rotation,\n",
    "                                                       num_available_measures=num_available_measures,\n",
    "                                                       weightings=['distance'])\n",
    "\n",
    "        for measure in updated:\n",
    "            df = k_reconstructions[measure]\n",
    "            if save:\n",
    "                df.to_pickle(basename[:-1]+'%s.pkl' % measure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
