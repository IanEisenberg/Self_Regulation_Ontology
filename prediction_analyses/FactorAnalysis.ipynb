{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a summary of the predictive analyses using task or survey data to predict demographic/health measures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,glob,sys\n",
    "import pickle\n",
    "import numpy,pandas\n",
    "pandas.options.display.max_colwidth = 0\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "%load_ext rpy2.ipython\n",
    "from scipy.cluster.hierarchy import dendrogram,ward,cut_tree,leaves_list\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import selfregulation.prediction.behavpredict_V1 as behavpredict\n",
    "\n",
    "clf='lasso'\n",
    "acc,features=pickle.load(open('singularity_analyses/wrangler/%s_data_collapsed.pkl'%clf,'rb'))\n",
    "cont_measure='r2' # use r^2 or MAE for non-binary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all variables to make sure they have the correct number of observations (1000 for baseline and baseline_shuffle, 100 for all others), and create tables summarizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data for emotion\n",
      "intelligence 5\n",
      "stroop 6\n",
      "nondecision 15\n",
      "stopping 14\n",
      "stop_signal 8\n",
      "bisbas 6\n",
      "grit 3\n",
      "discounting 8\n",
      "threebytwo 6\n",
      "attention_network_task 8\n",
      "motor_selective_stop_signal 2\n",
      "discount_titrate 3\n",
      "thresh 15\n",
      "risktaking 21\n",
      "baseline_shuffle 2\n",
      "baseline_shuffle DivorceCount 999 2\n",
      "tower_of_london 6\n",
      "dot_pattern_expectancy 9\n",
      "baseline 2\n",
      "kirby 5\n",
      "big5 7\n",
      "task 107\n",
      "task HowOftenGuiltRemorseDrinking 33 107\n",
      "task AlcoholHowOften6Drinks 40 107\n",
      "task HowOftenFailedActivitiesDrinking 24 107\n",
      "impulsivity 13\n",
      "survey 67\n",
      "drift 27\n",
      "columbia_card_task_hot 7\n"
     ]
    }
   ],
   "source": [
    "allvars={}\n",
    "datasets=[]\n",
    "for k in acc.keys():\n",
    "    if len(acc[k])==0:\n",
    "        print('no data for',k)\n",
    "        continue\n",
    "    datasets.append(k)\n",
    "    for v in acc[k][cont_measure]['scores_cv']:\n",
    "        allvars[v]=cont_measure\n",
    "    for v in acc[k]['AUROC']['scores_cv']:\n",
    "        allvars[v]='AUROC'\n",
    "\n",
    "alldata={'r2':pandas.DataFrame(),'MAE':pandas.DataFrame(),'AUROC':pandas.DataFrame(),\n",
    "        'r2_pval':pandas.DataFrame()}\n",
    "#['baseline_shuffle','baseline','task','survey','discounting',\n",
    "#          'stopping','intelligence',\n",
    "#          'impulsivity','big5','risktaking','grit','emotion','bisbas',\n",
    "#          'drift','nondecision','thresh']\n",
    "target_n={}\n",
    "goodcount={}\n",
    "for d in datasets:\n",
    "    if len(acc[k])==0:\n",
    "        print('no data for',k)\n",
    "        continue\n",
    "    \n",
    "    goodcount[d]={}\n",
    "    if d.find('baseline')>-1:\n",
    "        target_n[d]=1000\n",
    "    else:\n",
    "        target_n[d]=100\n",
    "    examplefeature=list(features[d].keys())[0]\n",
    "    print(d,features[d][examplefeature].shape[1])\n",
    "\n",
    "    for v in acc[d]['r2']['scores_cv']:\n",
    "        if not v in acc[d][allvars[v]]['scores_cv']:\n",
    "            goodcount[d][v]=0\n",
    "        else:\n",
    "            goodcount[d][v]=numpy.isfinite(acc[d][allvars[v]]['scores_cv'][v]).sum()\n",
    "        if goodcount[d][v]<target_n[d]:\n",
    "            print(d,v,goodcount[d][v],features[d][v].shape[1])\n",
    "\n",
    "for v in allvars:\n",
    "    if allvars[v]==cont_measure:\n",
    "        vars={}\n",
    "        for k in datasets:\n",
    "            if not 'r2' in acc[k]:\n",
    "                continue\n",
    "            vars[k]=acc[k]['r2']['scores_cv'].mean().T\n",
    "        df=pandas.DataFrame(vars,index=[v])\n",
    "        alldata['r2']=alldata['r2'].append(df)\n",
    "        \n",
    "        vars={}\n",
    "        for k in datasets:\n",
    "            if not 'MAE' in acc[k]:\n",
    "                continue\n",
    "            vars[k]=acc[k]['MAE']['scores_cv'].mean().T\n",
    "        df=pandas.DataFrame(vars,index=[v])\n",
    "        alldata['MAE']=alldata['MAE'].append(df)\n",
    "    else:\n",
    "        vars={}\n",
    "        for k in datasets:\n",
    "            if not 'AUROC' in acc[k]:\n",
    "                continue\n",
    "            vars[k]=acc[k]['AUROC']['scores_cv'].mean().T\n",
    "        df=pandas.DataFrame(vars,index=[v])\n",
    "        alldata['AUROC']=alldata['AUROC'].append(df)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Compute p values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intelligence\n",
      "stroop\n",
      "nondecision\n",
      "stopping\n",
      "stop_signal\n",
      "bisbas\n",
      "grit\n",
      "discounting\n",
      "threebytwo\n",
      "attention_network_task\n",
      "motor_selective_stop_signal\n",
      "discount_titrate\n",
      "thresh\n",
      "risktaking\n",
      "tower_of_london\n",
      "dot_pattern_expectancy\n",
      "kirby\n",
      "big5\n",
      "task\n",
      "impulsivity\n",
      "survey\n",
      "drift\n",
      "columbia_card_task_hot\n"
     ]
    }
   ],
   "source": [
    "target='baseline'\n",
    "null='baseline_shuffle'\n",
    "def get_pval(target,null,allvars,datasets,acc):\n",
    "    data=[]\n",
    "    vars=list(allvars.keys())\n",
    "    vars.sort()\n",
    "    for v in vars:\n",
    "        #print(target,null,v)\n",
    "        if not v in acc[target][allvars[v]]['scores_cv'] or not v in acc[null][allvars[v]]['scores_cv']:\n",
    "            data.append([allvars[v],numpy.nan,numpy.nan,numpy.nan,numpy.nan,numpy.nan])\n",
    "            continue\n",
    "        targdist=acc[target][allvars[v]]['scores_cv'][v].dropna()\n",
    "        targmean=targdist.mean()\n",
    "        nulldist=acc[null][allvars[v]]['scores_cv'][v].dropna()\n",
    "        nullmean=nulldist.mean()\n",
    "        targstd=targdist.std()\n",
    "        pval=1-scipy.stats.percentileofscore(nulldist,targmean)/100.\n",
    "        if targstd>0:\n",
    "            #es=(targmean-nullmean)/targstd\n",
    "            es=targmean-nullmean\n",
    "        else:\n",
    "            es=numpy.nan\n",
    "        insample=acc[target][allvars[v]]['scores_insample'][v].mean()\n",
    "        data.append([allvars[v],targmean,nullmean,es,insample,pval])\n",
    "    df=pandas.DataFrame(data,index=vars,columns=['Measure','Target mean','Null Mean','Effect size','In-sample','p_unc'])\n",
    "    return(df)\n",
    "        \n",
    "pvals={}\n",
    "pvals[('baseline','baseline_shuffle')]=get_pval('baseline','baseline_shuffle',allvars,datasets,acc)\n",
    "for d in datasets:\n",
    "    if d.find('baseline')>-1 or len(acc[d])==0:\n",
    "        continue\n",
    "    print(d)\n",
    "    pvals[(d,'baseline')]=get_pval(d,'baseline',allvars,datasets,acc)\n",
    "\n",
    "pvals_fdr={}\n",
    "for k in pvals:\n",
    "    tmp=multipletests(pvals[k]['p_unc'])\n",
    "    pvals[k]['p_fdr']=tmp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_importances(v,dt,features,nfeats=3):\n",
    "    if not v in features[dt]:\n",
    "        print(v,'is not in features for',dt)\n",
    "        return None\n",
    "    \n",
    "    #print(dt,'importances for:',v)\n",
    "    imp=pandas.DataFrame({'importance':(features[dt][v].abs()>0).mean(0)})\n",
    "    imp['mean']=features[dt][v].mean(0)\n",
    "    imp=imp.sort_values(by='importance',ascending=False)\n",
    "    \n",
    "    if nfeats>(imp.shape[0]):\n",
    "        nfeats=imp.shape[0]\n",
    "    topfeats=imp.iloc[:nfeats]\n",
    "    topfeats=topfeats.query('importance>0')\n",
    "    return topfeats\n",
    "\n",
    "def get_importance_list(sigp,dt,features):\n",
    "    implist=[]\n",
    "    for v in sigp.index:\n",
    "        i=get_importances(v,dt,features)\n",
    "        implist.append([list(i.index)])\n",
    "    df=pandas.DataFrame({'top features':implist})\n",
    "    df.index=sigp.index\n",
    "    return df\n",
    "\n",
    "# plot var for all datasets\n",
    "def plotvars(v,pvals,datasets,allvars,plotcutoff=True,\n",
    "            plotbaseline=False):\n",
    "\n",
    "    df=[]\n",
    "    errors=[]\n",
    "    ds=[]\n",
    "    for k in datasets:\n",
    "        if not allvars[v] in acc[k]:\n",
    "            continue\n",
    "        if not v in acc[k][allvars[v]]['scores_cv']:\n",
    "            continue\n",
    "        targdist=acc[k][allvars[v]]['scores_cv'][v].dropna()\n",
    "        df.append(targdist.mean())\n",
    "        ds.append(k)\n",
    "        errors.append(targdist.std())\n",
    "    df=pandas.DataFrame({'mean':df},index=ds)\n",
    "    errors=pandas.DataFrame({'mean':errors},index=ds)\n",
    "    if allvars[v]=='AUROC':\n",
    "        df.plot.bar(yerr=errors,legend=False,ylim=(0.45,numpy.max(df.values)*1.1))\n",
    "    else:\n",
    "        df.plot.bar(yerr=errors,legend=False)\n",
    "    plt.title(v)\n",
    "    plt.ylabel(allvars[v]+' +/- SE across CV runs')\n",
    "    if plotcutoff:\n",
    "        cutoff=acc['baseline'][allvars[v]]['scores_cv'][v].dropna().quantile(0.95)\n",
    "        plt.plot([0,1000],[cutoff,cutoff],'k--',linewidth=0.5)\n",
    "    if plotbaseline:\n",
    "        baseline=acc['baseline'][allvars[v]]['scores_cv'][v].dropna().mean()\n",
    "        plt.plot([0,1000],[baseline,baseline],'k--',linewidth=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess survey variables in terms of their overall predictive utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k=('survey','baseline')\n",
    "df=pandas.DataFrame()\n",
    "absfeat=pandas.DataFrame()\n",
    "\n",
    "for v in features['survey']:\n",
    "    df[v]=features['survey'][v].mean(0)\n",
    "    absfeat[v]=(features['survey'][v].abs()>0).mean()\n",
    "    \n",
    "mean_imp=df.mean(1)\n",
    "meanabs_survey=pandas.DataFrame({'meanabs':absfeat.mean(1)}).sort_values(by='meanabs',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pandas.DataFrame()\n",
    "absfeat=pandas.DataFrame()\n",
    "\n",
    "for v in features['task']:\n",
    "    df[v]=features['task'][v].mean(0)\n",
    "    absfeat[v]=(features['task'][v].abs()>0).mean()\n",
    "    \n",
    "mean_imp=df.mean(1)\n",
    "meanabs_task=pandas.DataFrame({'meanabs':absfeat.mean(1)}).sort_values(by='meanabs',ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize structure of demographic target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacing bad WeightPounds value for Index(['s028'], dtype='object')\n",
      "replacing bad HeightInches value for Index(['s462', 's513', 's517'], dtype='object')\n",
      "replacing bad CaffienatedSodaCansPerDay value for Index(['s108'], dtype='object')\n",
      "dropping categorical variable: HispanicLatino\n",
      "dropping categorical variable: Race\n",
      "dropping categorical variable: DiseaseDiagnoses\n",
      "dropping categorical variable: DiseaseDiagnosesOther\n",
      "dropping categorical variable: MotivationForParticipation\n",
      "dropping categorical variable: MotivationOther\n",
      "dropping categorical variable: NeurologicalDiagnoses\n",
      "dropping categorical variable: NeurologicalDiagnosesDescribe\n",
      "dropping categorical variable: OtherDebtSources\n",
      "dropping categorical variable: OtherDrugs\n",
      "dropping categorical variable: OtherRace\n",
      "dropping categorical variable: OtherTobaccoProducts\n",
      "dropping categorical variable: PsychDiagnoses\n",
      "dropping categorical variable: PsychDiagnosesOther\n",
      "dropping skipped variable: RetirementPercentStocks\n",
      "dropping skipped variable: HowOftenFailedActivitiesDrinking\n",
      "dropping skipped variable: HowOftenGuiltRemorseDrinking\n",
      "dropping skipped variable: AlcoholHowOften6Drinks\n"
     ]
    }
   ],
   "source": [
    "bp=behavpredict.BehavPredict(verbose=True,\n",
    "     drop_na_thresh=100,\n",
    "     skip_vars=['RetirementPercentStocks',\n",
    "     'HowOftenFailedActivitiesDrinking',\n",
    "     'HowOftenGuiltRemorseDrinking',\n",
    "     'AlcoholHowOften6Drinks'],\n",
    "     add_baseline_vars=True,\n",
    "     freq_threshold=0.1)\n",
    "bp.load_demog_data()\n",
    "bp.get_demogdata_vartypes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing GamblingProblem\n",
      "removing TrafficTicketsLastYearCount\n",
      "removing HowOftenDrinkMorning\n",
      "removing HowOftenCantStopCannabis\n",
      "removing HowOftenFailedActivitiesCannabis\n",
      "removing HowOftenDevotedTimeCannabis\n",
      "removing HowOftenMemoryConcentrationProblemCannabis\n",
      "removing HowOftenHazardousCannabis\n",
      "removing CannabisConsideredReduction\n",
      "removing AbuseMoreThanOneDrugAtATime\n",
      "removing BlackoutFlashbackDrugUse\n",
      "removing FeelBadGuiltyDrugUse\n",
      "removing SpouseParentsComplainDrugUse\n",
      "removing NeglectedFamilyDrugUse\n",
      "removing EngagedInIllegalActsToObtainDrugs\n",
      "removing WidthdrawalSymptoms\n",
      "removing MedicalProblemsDueToDrugUse\n",
      "removing DoctorVisitsLastMonth\n",
      "(51, 523)\n",
      "removing HowOftenCantStopDrinking\n"
     ]
    }
   ],
   "source": [
    "demogdata=bp.demogdata.copy()\n",
    "for i in demogdata.columns:\n",
    "    if not i in features['task'] and not i in features['survey']:\n",
    "        del demogdata[i]\n",
    "        print('removing',i)\n",
    "demogdata=demogdata.T\n",
    "demogdata['goodvar']=demogdata.isnull().sum(1)<10\n",
    "demogdata_clean=demogdata.query('goodvar==True')\n",
    "print(demogdata.shape)\n",
    "del demogdata_clean['goodvar']\n",
    "demogdata_clean=demogdata_clean.T\n",
    "\n",
    "# these are bad vars that don't have features\n",
    "dropvars=['HowOftenCantStopDrinking',\n",
    "'HowOftenFailedActivitiesDrinking',\n",
    "'HowOftenGuiltRemorseDrinking','AlcoholHowOften6Drinks']\n",
    "\n",
    "for v in dropvars:\n",
    "    if v in demogdata_clean:\n",
    "        del demogdata_clean[v]\n",
    "        print('removing',v)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from fancyimpute import SimpleFill\n",
    "\n",
    "def residualize_baseline(df):\n",
    "    # remove baseline vars\n",
    "    baseline=df[['Age','Sex']]\n",
    "    data=df.copy()\n",
    "    del data['Age']\n",
    "    del data['Sex']\n",
    "    #x=SimpleFill().complete(baseline)\n",
    "    lr=LinearRegression()\n",
    "    for v in data:\n",
    "        #print('residualizing',v)\n",
    "        if data[v].isnull().sum()>0:\n",
    "            y=SimpleFill().complete(data[v].values[:,numpy.newaxis])\n",
    "        else:\n",
    "            y=data[v]\n",
    "        lr.fit(baseline,y)\n",
    "        data[v]=y - lr.predict(baseline)\n",
    "    return data\n",
    "df_resid=residualize_baseline(demogdata_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_resid.to_csv('../Data/Derived_Data/Complete_10-08-2017/demog_residAgeSex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['HighestEducation', 'HeightInches', 'WeightPounds'],\n",
       " 1: ['RelationshipStatus', 'DivorceCount', 'LongestRelationship'],\n",
       " 2: ['RelationshipNumber', 'ChildrenNumber', 'HouseholdIncome'],\n",
       " 3: ['RetirementAccount', 'RentOwn', 'CoffeeCupsPerDay'],\n",
       " 4: ['TeaCupsPerDay',\n",
       "  'CaffienatedSodaCansPerDay',\n",
       "  'CaffieneOtherSourcesDayMG'],\n",
       " 5: ['TrafficAccidentsLifeCount', 'ArrestedChargedLifeCount'],\n",
       " 6: ['LifetimeSmoke100Cigs',\n",
       "  'HowLongSmoked',\n",
       "  'SmokeEveryDay',\n",
       "  'CigsPerDay',\n",
       "  'HowSoonSmokeAfterWaking'],\n",
       " 7: ['AlcoholHowOften',\n",
       "  'AlcoholHowManyDrinksDay',\n",
       "  'HowOftenUnableRememberDrinking'],\n",
       " 8: ['InjuredDrinking',\n",
       "  'RelativeFriendConcernedDrinking',\n",
       "  'CannabisPast6Months',\n",
       "  'AbleToStopDrugs'],\n",
       " 9: ['Nervous',\n",
       "  'Hopeless',\n",
       "  'RestlessFidgety',\n",
       "  'Depressed',\n",
       "  'EverythingIsEffort',\n",
       "  'Worthless',\n",
       "  'Last30DaysUsual'],\n",
       " 10: ['BMI', 'Obese']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dthresh=2.0\n",
    "dist=1-numpy.abs(df_resid.corr(method='spearman'))\n",
    "k=ward(numpy.triu(dist))\n",
    "c=cut_tree(k,height=dthresh)\n",
    "ll=leaves_list(k)\n",
    "\n",
    "matches={}\n",
    "matchnums={}\n",
    "clustdict={}\n",
    "for i in numpy.unique(c):\n",
    "    matches[i]=[]\n",
    "    matchnums[i]=[]\n",
    "    for j in numpy.where(c==i)[0]:\n",
    "        matches[i].append(df_resid.columns[j])\n",
    "        clustdict[df_resid.columns[j]]=i\n",
    "        matchnums[i].append(j)\n",
    "\n",
    "matchdesc={0:'education/height/weight',1:'relationships',2:'domestic',3:'financial/coffee',\n",
    "          4:'caffeine',5:'legal problems',6:'smoking',7:'alcohol use',\n",
    "          8:'alcohol/drug problems',9:'mental health',10:'obesity'}\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering on predictor loadings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks pretty crappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveyfiles=glob.glob('/Users/poldrack/code/Self_Regulation_Ontology/prediction_analyses/R_exports_lasso/features/survey*')\n",
    "dropvars=['Age','Sex']\n",
    "loadingdata={'survey':None}\n",
    "include_task=False\n",
    "\n",
    "for f in surveyfiles:\n",
    "    varname=f.split('survey')[1].split('_')[1]\n",
    "    for d in dropvars:\n",
    "        if f.find(d)>-1:\n",
    "            continue\n",
    "    sdata=pandas.read_csv(f).mean(0)\n",
    "    if include_task:\n",
    "        tf=f.replace('features/survey_','features/task_')\n",
    "        if not os.path.exists(tf):\n",
    "            print('skipping',varname)\n",
    "            continue\n",
    "        tdata=pandas.read_csv(tf).mean(0)\n",
    "        alldata=pandas.concat((tdata,sdata))\n",
    "    else:\n",
    "        alldata=sdata\n",
    "    if loadingdata['survey'] is None:\n",
    "        loadingdata['survey']=pandas.DataFrame({varname:alldata})\n",
    "    else:\n",
    "        loadingdata['survey'][varname]=alldata\n",
    "        \n",
    "loadingdata['survey']=loadingdata['survey'].drop('Age').drop('Sex')\n",
    "del loadingdata['survey']['Age']\n",
    "del loadingdata['survey']['Sex']\n",
    "allvars=[i for i in list(loadingdata['survey'].columns) if not i.find('.binarized')>-1]\n",
    "for c in allvars:\n",
    "    if '%s.binarized'%c in loadingdata['survey']:\n",
    "        del loadingdata['survey']['%s.binarized'%c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor analysis on outcome measures \n",
    "Exploratory - don't use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i df_resid -o scores,loadings,varnames\n",
    "\n",
    "\n",
    "dropvars <- names(df_resid) %in% c(\"HeightInches\", \"WeightPounds\", \"CigsPerDay\") \n",
    "print(dropvars)\n",
    "df <- df_resid[,!dropvars]\n",
    "\n",
    "\n",
    "library(psych)\n",
    "library(semPlot)\n",
    "vss.result=VSS(df,16,fm='mle',plot=FALSE)\n",
    "#print(vss.result)\n",
    "nfactor=which.min(vss.result$vss.stats$BIC)\n",
    "fa.result=fa(df,nfactors=nfactor,fm='mle')\n",
    "loadings=fa.result$loadings\n",
    "print(fa.result,cut=0.2,sort=TRUE)\n",
    "scores=factor.scores(df,fa.result,method='tenBerge')$scores\n",
    "semPaths(fa.result)\n",
    "#clst=iclust(df_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_df=pandas.DataFrame(scores,columns=['smoking severity','mental illness',\n",
    "                                           'smoking','obesity',\n",
    "                                           'alcohol','domestic'],index=df_resid.index)\n",
    "scores_df.to_csv(\"../Data/Derived_Data/Complete_10-08-2017/factor_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(psych)\n",
    "library(MASS)\n",
    "library(semPlots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
